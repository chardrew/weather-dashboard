x-spark-common: &spark-common
  image: bitnami/spark:${SPARK_VERSION}
  env_file:
    - .env
  volumes:
    - ./jobs:/opt/bitnami/spark/jobs

x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: dockerfiles/Dockerfile.airflow
  env_file:
    - .env
  environment:
    AIRFLOW__CORE__LOAD_EXAMPLES: False
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DATABASE_USER}:${AIRFLOW_DATABASE_PASSWORD}@${AIRFLOW_DATABASE_HOST}:${AIRFLOW_DATABASE_PORT}/${AIRFLOW_DATABASE_NAME}
    AIRFLOW__WEBSERVER_BASE_URL: http://localhost:8080
    AIRFLOW__WEBSERVER__SECRET_KEY: 46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
    AIRFLOW__CORE__FERNET_KEY: AK32hnwcav-SbKx4RHSfy5qH9V4bYkqtAdvdAvOmxNA=
    PYTHONPATH: /opt/airflow:/opt/airflow/plugins
  volumes:
    - ./config:/opt/airflow/config
    - ./dags:/opt/airflow/dags
    - ./jobs:/opt/airflow/jobs
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./scripts:/opt/airflow/scripts
  depends_on:
    - airflow-postgres

services:
  # real-time dashboard
  websocket-server:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile.websocket
    container_name: websocket-server
    env_file:
      - ./.env
    environment:
      PYTHONPATH: /app
    volumes:
      - ./config/properties.py:/app/config/properties.py
      - ./scripts/websocket_server.py:/app/scripts/websocket_server.py
    depends_on:
      weather-postgres:
        condition: service_healthy
    networks:
    - ${WEATHER_NETWORK_NAME}

  streamlit-dashboard:
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile.dashboard
    container_name: dashboard
    env_file:
      - ./.env
    environment:
      PYTHONPATH: /app
    volumes:
      - ./config/properties.py:/app/config/properties.py
      - ./scripts/dashboard.py:/app/scripts/dashboard.py
    depends_on:
      - websocket-server
      - weather-postgres
    ports:
      - "${STREAMLIT_DASHBOARD_PORT}:8501"
    networks:
      - ${WEATHER_NETWORK_NAME}

  # database
  weather-postgres:
    image: postgres:${DATABASE_VERSION}
    container_name: weather-postgres
    environment:
      POSTGRES_DB: ${DATABASE_NAME}
      POSTGRES_USER: ${DATABASE_USER}
      POSTGRES_PASSWORD: ${DATABASE_PASSWORD}
    healthcheck:
      test: [ 'CMD-SHELL', 'pg_isready -U ${DATABASE_USER} -d ${DATABASE_NAME}' ]
      interval: 10s
      timeout: 5s
      retries: 3
    volumes:
      - postgres_weather_data:/var/lib/postgresql/data
    networks:
      - ${WEATHER_NETWORK_NAME}
    restart: always

  # kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:${ZOOKEEPER_VERSION}
    container_name: zookeeper
    env_file:
      - .env
    environment:
      ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_CLIENT_PORT}
    healthcheck:
      test: [ "CMD-SHELL", "echo srvr | nc -w 2 zookeeper ${ZOOKEEPER_CLIENT_PORT} || exit 1" ]
      interval: 10s
      timeout: 5s
    networks:
      - ${WEATHER_NETWORK_NAME}

  kafka:
    image: confluentinc/cp-kafka:${KAFKA_VERSION}
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    env_file:
      - .env
    healthcheck:
      test: [ "CMD-SHELL", "nc -z ${KAFKA_HOST_APP} ${KAFKA_PORT_APP} || exit 1" ]
      interval: 10s
      timeout: 5s
    environment:
      KAFKA_BROKER_ID: ${KAFKA_BROKER_ID}
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:${ZOOKEEPER_CLIENT_PORT}
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://${KAFKA_HOST_APP}:${KAFKA_PORT_APP}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: ${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR}
    networks:
      - ${WEATHER_NETWORK_NAME}

   # spark
  spark-master:
    <<: *spark-common
    container_name: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "${SPARK_MASTER_WEB_UI_PORT}:8080"
    networks:
      - ${WEATHER_NETWORK_NAME}

  spark-worker:
    <<: *spark-common
    container_name: spark-worker
    command: bin/spark-class org.apache.spark.deploy.worker.Worker ${SPARK_USER}://${SPARK_HOST}:${SPARK_PORT}
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: ${SPARK_WORKER_CORES}
      SPARK_WORKER_MEMORY: ${SPARK_WORKER_MEMORY}
      SPARK_MASTER_URL: ${SPARK_USER}://${SPARK_HOST}:${SPARK_PORT}
    networks:
      - ${WEATHER_NETWORK_NAME}

  # airflow
  airflow-postgres:
    image: postgres:${DATABASE_VERSION}
    container_name: ${AIRFLOW_DATABASE_HOST}
    environment:
      POSTGRES_DB: ${AIRFLOW_DATABASE_NAME}
      POSTGRES_USER: ${AIRFLOW_DATABASE_USER}
      POSTGRES_PASSWORD: ${AIRFLOW_DATABASE_PASSWORD}
    healthcheck:
      test: [ 'CMD-SHELL', 'pg_isready -U ${AIRFLOW_DATABASE_USER} -d ${AIRFLOW_DATABASE_NAME}' ]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - ${WEATHER_NETWORK_NAME}

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    depends_on:
      airflow-postgres:
        condition: service_healthy
    entrypoint: [ "/bin/bash", "/opt/airflow/scripts/airflow-init.sh" ]
    restart: 'no'
    networks:
      - ${WEATHER_NETWORK_NAME}

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    entrypoint: airflow webserver
    ports:
      - "${AIRFLOW_WEBSERVER_PORT}:8080"
    depends_on:
      - airflow-scheduler
    restart: always
    networks:
      - ${WEATHER_NETWORK_NAME}

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    depends_on:
      - airflow-init
    entrypoint: airflow scheduler
    restart: always
    networks:
      - ${WEATHER_NETWORK_NAME}

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    depends_on:
      - airflow-webserver
      - airflow-scheduler
    command: airflow triggerer
    restart: unless-stopped
    networks:
      - ${WEATHER_NETWORK_NAME}

  # weather data api
  get_weather_dev: # runs only when 'docker-compose --profile dev' up is run
    container_name: get-weather-dev
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile.weather_api
      args:
        PYTHON_VERSION: ${PYTHON_VERSION}
    env_file:
      - .env
    environment:
      PYTHONPATH: /app
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./config/properties.py:/app/config/properties.py
      - ./scripts/get_weather_sim.py:/app/scripts/get_weather_sim.py
    networks:
      - ${WEATHER_NETWORK_NAME}
    entrypoint: [ "python", "/app/scripts/get_weather_sim.py" ]
    profiles:
      - dev

  get_weather_prod:  # runs only when 'docker-compose --profile prod' up is run
    container_name: get-weather-prod
    build:
      context: .
      dockerfile: dockerfiles/Dockerfile.weather_api
      args:
        PYTHON_VERSION: ${PYTHON_VERSION}
    env_file:
      - .env
    environment:
      PYTHONPATH: /app
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ./config/properties.py:/app/config/properties.py
      - ./scripts/get_weather.py:/app/scripts/get_weather.py
    networks:
      - ${WEATHER_NETWORK_NAME}
    entrypoint: [ "python", "/app/scripts/get_weather.py" ]
    profiles:
      - prod


networks:
  airflow_network:
    name: airflow_network

volumes:
  postgres_weather_data:  # for database persistence
